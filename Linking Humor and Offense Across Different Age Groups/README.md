Project realizing SemEval-2021 Task 7: Hahackathon: Linking Humor and Offense using Bert model

# Linking Humor and Offense Across Different Age Groups

https://competitions.codalab.org/competitions/27446

Task 1 emulates previous humor detection tasks in which all ratings were averaged to provide mean classification and rating scores. 

-	Task 1a: predict if the text would be considered humorous (for an average user). This is a binary task.

-	Task 1b: if the text is classed as humorous, predict how humorous it is (for an average user). The values vary between 0 and 5.

-	Task 1c: if the text is classed as humorous, predict if the humor rating would be considered controversial, i.e. the variance of the rating between annotators is higher than the median. This is a binary task.


To complete the task, I decided to use the BERT[[1](https://arxiv.org/pdf/1810.04805.pdf)],[[2](https://github.com/google-research/bert)] tool created by Google. BERT is designed for bidirectional deep learning pre-conditioning by co-conditioning both left and right contexts across all layers. BERT uses a neural network transformer architecture, so parallelism can be very helpful, while others (ELMO and ULMfit) use LSTM. BERT has state of the art results in many NLP tasks. It is a bi-directional transformer pre-trained, so we get results very quickly, even when using only one layer. This greatly affects the ease of experimentation.
1. The input data has been loaded and split into columns.
2. In order to create a neural network, I used Keras - a deep learning API written in Python, running on the TensorFlow machine learning platform.

I used a pre-trained bert-base-uncased [[3](https://huggingface.co/bert-base-uncased)] model that was trained. With a data set of labeled sentences, it allows training a classifier using the features generated by the BERT model as input data.

The neural network consists of 2 layers with a size of 16 (32 is also recommended, but for performance reasons I had to choose 16), with a maximum length of 200.

##### Summary
When using a pre-trained model, the biggest advantage is the quick achievement of satisfactory results and the ease of use of this tool.

One disadvantage is the amount of computational resources needed to train/tune the model. Working on Google colabolatory, I used an input of size 16 because there were memory issues with 32.

Possible changes:
- Adding a layer of attention.
- Testing on a larger or Bert large model.
